---
title: "Bayesian linear regression"
author: Dr Merlise A Clyde, Duke University
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
myblue = rgb(86,155,189, name="myblue", max=256)
mydarkgrey = rgb(.5,.5,.5, name="mydarkgrey", max=1)
```
We illustrate fitting a Bayesian version of the simple linear regression model using the `bodyfat` data from the library `BAS`.    

```{r}
library(BAS)
data(bodyfat)
summary(bodyfat)
```
This includes 252 measurements on men of body fat and other measurements, such as waist circumference.
The plot of bodyfat versus the waist circumference below

### Scatterplot and OLS line ###
```{r}
bodyfat.lm = lm(Bodyfat ~ Abdomen, data=bodyfat)

plot(Bodyfat ~ Abdomen, data=bodyfat, 
     xlab="abdomen circumference (cm)", 
     col=myblue, pch=16, main="")
beta = coef(bodyfat.lm)
abline(beta, lwd=4, col=1)
```

suggests that a linear model may be a reasonable approximation within this range of data.

### Posterior distribution using the reference prior###

We will use `lm` to obtain the OLS estimates which provide the posterior mean and standard deviation.  The `confint` function provides 95% confidence intervals, which under the reference prior are equivalent to 95% credible intervals.  The code below extracts them and just relabels the output.


```{r}
summary(bodyfat.lm)

out = summary(bodyfat.lm)$coef[, 1:2]
out = cbind(out, confint(bodyfat.lm))
colnames(out) = c("posterior mean", "sd", "2.5", "97.5")
round(out, 2)
```

### credible intervals for the mean and prediction ###

```{r}
x = bodyfat$Abdomen
y= bodyfat$Bodyfat
xnew <- seq(min(x), max(x), length.out = 100)
ynew <- data.frame(predict(bodyfat.lm, newdata = data.frame(Abdomen = xnew), 
                   interval = "confidence", level = 0.95))
plot(x,y, xlab = "abdomen", ylab="bodyfat", col=myblue, pch=16)
lines(ynew$lwr ~ xnew, lty = 2, lwd=3, col=mydarkgrey)
lines(ynew$upr ~ xnew, lty = 2, lwd=3, col=mydarkgrey)
abline(bodyfat.lm, col="orange")
ynew <- data.frame(predict(bodyfat.lm, newdata = data.frame(Abdomen = xnew), 
                   interval = "prediction", level = 0.95))
lines(ynew$lwr ~ xnew, lty = 3, lwd=3, col=mydarkgrey)
lines(ynew$upr ~ xnew, lty = 3, lwd=3, col=mydarkgrey)
points(bodyfat[39,"Abdomen"], bodyfat[39,"Bodyfat"], col="orange", cex=5)
legend(110,15, legend=c("Posterior mean", "95% CI for mean", "95% CI for predictions"), 
       col=c("orange",rep(mydarkgrey, 2)), lwd=3, lty=c(1,2, 3))

```

###Outliers###

The plot and predictive intervals suggest that predictions for Case 39 are not well captured by the model. There is always the possibility that this case does not meet the assumptions of the simple linear model (wrong mean or variance) or could be in error.  Model diagnostics such as plots of  residuals versus fitted are useful in identifying potiential outliers, but we can also use the Bayesian paradigm to go further.

The article by Chaloner & Brant (1988)  suggested on approach for defining outliers and then calculating the probability that an case or multiple cases were outliers.  The assumed model for our simple linear regression is
$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ with $\epsilon_$ having independent, identical distributions that are normal with mean zero 
and constant variance $\sigma^2$, $N(0, \sigma^2)$.   Chaloner & Brant consider outliers to be points where the error or model discrepancy
$\epsilon_i$ is greater than $\kappa$ standard deviations in either direction from zero, or the event  $|\epsilon| > \kappa \sigma$,
and then proceed to calculate the posterior probability that a case is an outlier as $P(|\epsilon| > \kappa \sigma \mid data)$.  
Since $Y_i - \beta_0 - \beta_1 = \epsilon_i$, this is equivalent to $P(|Y_i - \beta_0 - \beta_1| > \kappa \sigma \mid data)$.  



This code for calculating the probability of outliers involves integration (for more details see the paper), so to simply life, we have  implemented this in the function `Bayes.outlier.prob` that can be sourced from the file `bayes-outliers.R`.   Feel free to dig into the code!   
Applying this to the body fat data for case 39, 
```{r Bayes Outliers}
source("bayes-outliers.R")
library(mvtnorm)
outliers= Bayes.outlier.prob(bodyfat.lm) 
#The default is to consider k=3 standard deviations.
prob.39 = outliers$prob.outlier[39]
prob.39

```
we see that this case has an extremely high probability (`r prob.39`) of being more an outlier, that is the error is greater than $k=3$ standard deviations, based on the fitted model and data.


With $\kappa = 3$, however, there may be a high probablity a priori of at least one outlier in a large sample.  We can compute this using
```{r at least one}
n = nrow(bodyfat)
# probability of no outliers
(1 - (2*pnorm(-3)))^n
# probability of at least one outlier
1 - (1 - (2*pnorm(-3)))^n
```
With n =  `r n`,  the probability of at least one outlier is much larger than say the marginal probability that one point is an outlier of 0.05 and we would expect that there will be at least one point where the error is more than 3 standard deviations from zero almost 50% ot the time.
Rather than fix $\kappa$, we can fix the prior probability of no outliers to be say 0.95 and solve for a value of $\kappa$.   
```{r k}
k = qnorm(.5 + .5*.95^(1/n))
k
```
This leads to a larger value of $\kappa$ after adjusting for the looking at there being at least one outlier and
```{r adjusted prob}
outliers.no= Bayes.outlier.prob(bodyfat.lm, k=k)
prob.no.39 = outliers.no$prob.outlier[39]
prob.no.39
```
a posterior probbility of case 39 being an outlier of `r prob.no.39`.
While this is not strikingly large it is much larger than the marginal prior probability of
```{r}
2*pnorm(-k)
```

## Summary

There is a substantial probability that case 39 is an outlier.  If you do view it as an outlier, what are your options?  One is to investigate that case and determine if the data are in error (data entry error) and fix it.  Another if you cannot confirm there is a data error, is to delete the observation from the analysis and refit the model without that case.  If you do take this option, be sure to describe what you did so that your research is reproducible.  You may want to apply diagnostics and calculate the probability of a case being an outlier using this reduced data. As a word of caution,  if you discover that there are a large number of points that appear to be outliers, take a second look at your model assumptions as the problem may be with the model rather than the data!  A third option that we will talk about later is to combine inference under the model that retains this case as part of the population and the model that treats it as coming from another population.   This approach incorporates our uncertainty about whether the case an outlier given the data.

This code is based on using a reference prior for the linear model and extends to multiple regression.

suggesting that case 39 is an outlier.