decision making under uncertainty
================

Load the data and preprocess

``` r
library(MASS)
data(UScrime)

#Log transform all continuous variables except `So` which is in column 2. 
# We're overwriting the dataframe in this case.

UScrime[,-2] = log(UScrime[,-2])
```

### Run BAS

I am going to run `BAS` using the sampling without replacement option to enumerate all \(2^15\) models.

``` r
library(BAS)
crime.ZS =  bas.lm(y ~ ., 
                   data=UScrime,
                   prior="ZS-null",
                   modelprior=uniform()) 
```

**Model Choice**

`BAS` has methods defined to return fitted values, `fitted`, using the observed design matrix and predictions at either the observed data or potentially new values, `predict`, as with `lm`.

``` r
muhat.BMA = fitted(crime.ZS, estimator="BMA")
BMA  = predict(crime.ZS, estimator="BMA")

# predict has additional slots for fitted values under BMA, predictions under each model
names(BMA)
```

    ##  [1] "fit"         "Ybma"        "Ypred"       "postprobs"   "se.fit"     
    ##  [6] "se.pred"     "se.bma.fit"  "se.bma.pred" "df"          "best"       
    ## [11] "bestmodel"   "prediction"  "estimator"

Plotting the two sets of fitted values,

``` r
par(mar=c(9, 9, 3, 3))
plot(muhat.BMA, BMA$fit, 
     pch=16, col=myblue,
     xlab=expression(hat(mu[i])), ylab=expression(hat(Y[i])))
abline(0,1)
```

![](5.4.4_decisions_files/figure-markdown_github/unnamed-chunk-1-1.png) we see that they are in perfect agreement. That is always the case as the posterior mean for the regression mean function at a point \(x\) is the expected posterior predictive value for \(Y\) at \(x\). This is true not only for estimators such as BMA, but the expected values under model selection.

### Inference with model selection

In addition to using BMA, we can use the posterior means under model selection. This corresponds to a decision rule that combines estimation and selection. `BAS` currently implements the following options

**highest probability model:**

``` r
HPM = predict(crime.ZS, estimator="HPM")

# show the indices of variables in the best model where 0 is the intercept
HPM$bestmodel
```

    ## [1]  0  1  3  4  9 11 13 14 15

A little more interpretable version with names:

``` r
(crime.ZS$namesx[HPM$bestmodel +1])[-1]
```

    ## [1] "M"    "Ed"   "Po1"  "NW"   "U2"   "Ineq" "Prob" "Time"

**median probability model:**

``` r
MPM = predict(crime.ZS, estimator="MPM")
(crime.ZS$namesx[attr(MPM$fit, 'model') +1])[-1]
```

    ## [1] "M"    "Ed"   "Po1"  "NW"   "U2"   "Ineq" "Prob"

Note that we can also extract the best model from the attribute in the fitted values as well.

**best predictive model:**

This is the model that is closest to BMA predictions under squared error loss.

``` r
BPM = predict(crime.ZS, estimator="BPM")
(crime.ZS$namesx[attr(BPM$fit, 'model') +1])[-1]
```

    ##  [1] "M"    "So"   "Ed"   "Po1"  "Po2"  "M.F"  "NW"   "U2"   "Ineq" "Prob"

Let's see how they compare:

``` r
myblue = rgb(86,155,189, name="myblue", max=256)
mydarkgrey = rgb(.5,.5,.5, name="mydarkgrey", max=1)
par(cex=1.8, cex.axis=1.8, cex.lab=2, mfrow=c(2,2), mar=c(5, 5, 3, 3), col.lab=mydarkgrey, col.axis=mydarkgrey, col=mydarkgrey)
library(GGally)
ggpairs(data.frame(HPM = as.vector(HPM$fit),  #this used predict so we need to extract fitted values
                   MPM = as.vector(MPM$fit),  # this used fitted
                   BPM = as.vector(BPM$fit),  # this used fitted
                   BMA = as.vector(BMA$fit))) # this used predict
```

![](5.4.4_decisions_files/figure-markdown_github/unnamed-chunk-6-1.png)

Using the `se.fit = TRUE` option with `predict` we can also calculate standard deviations for prediction or for the mean and use this as imput for the `confint` function for the prediction object.

``` r
BPM = predict(crime.ZS, estimator="BPM", se.fit=TRUE)
crime.conf.fit = confint(BPM, parm="mean")
crime.conf.pred = confint(BPM, parm="pred")
cbind(BPM$fit, crime.conf.fit, crime.conf.pred)
```

    ##                  2.5  %  97.5  %   2.5  %  97.5  %
    ##  [1,] 6.668988 6.513238 6.824738 6.258715 7.079261
    ##  [2,] 7.290854 7.151787 7.429921 6.886619 7.695089
    ##  [3,] 6.202166 6.039978 6.364354 5.789406 6.614926
    ##  [4,] 7.661307 7.490608 7.832006 7.245129 8.077484
    ##  [5,] 7.015570 6.847647 7.183493 6.600523 7.430617
    ##  [6,] 6.469547 6.279276 6.659818 6.044966 6.894128
    ##  [7,] 6.776133 6.555130 6.997135 6.336920 7.215346
    ##  [8,] 7.299560 7.117166 7.481955 6.878450 7.720670
    ##  [9,] 6.614927 6.482384 6.747470 6.212890 7.016964
    ## [10,] 6.596912 6.468988 6.724836 6.196374 6.997449
    ## [11,] 7.032834 6.877582 7.188087 6.622750 7.442918
    ## [12,] 6.581822 6.462326 6.701317 6.183896 6.979748
    ## [13,] 6.467921 6.281998 6.653843 6.045271 6.890571
    ## [14,] 6.566239 6.403813 6.728664 6.153385 6.979092
    ## [15,] 6.550129 6.388987 6.711270 6.137779 6.962479
    ## [16,] 6.888592 6.746097 7.031087 6.483166 7.294019
    ## [17,] 6.252735 6.063944 6.441526 5.828815 6.676654
    ## [18,] 6.795764 6.564634 7.026895 6.351369 7.240160
    ## [19,] 6.945687 6.766289 7.125086 6.525866 7.365508
    ## [20,] 7.000331 6.840374 7.160289 6.588442 7.412220
    ## [21,] 6.613748 6.443389 6.784108 6.197710 7.029787
    ## [22,] 6.509534 6.352123 6.666946 6.098628 6.920441
    ## [23,] 6.781430 6.589687 6.973172 6.356187 7.206672
    ## [24,] 6.801865 6.659905 6.943825 6.396626 7.207104
    ## [25,] 6.368493 6.187973 6.549014 5.948191 6.788795
    ## [26,] 7.406220 7.173560 7.638879 6.961027 7.851412
    ## [27,] 5.995056 5.780243 6.209869 5.558924 6.431187
    ## [28,] 7.130996 6.970370 7.291621 6.718847 7.543144
    ## [29,] 7.084303 6.904331 7.264275 6.664237 7.504370
    ## [30,] 6.519208 6.360876 6.677539 6.107948 6.930468
    ## [31,] 6.191546 5.952977 6.430114 5.743237 6.639854
    ## [32,] 6.646586 6.472328 6.820844 6.228936 7.064236
    ## [33,] 6.778853 6.591383 6.966323 6.355520 7.202186
    ## [34,] 6.813627 6.683297 6.943958 6.412314 7.214940
    ## [35,] 6.686652 6.503099 6.870205 6.265039 7.108265
    ## [36,] 7.046639 6.788852 7.304426 6.587815 7.505464
    ## [37,] 6.786861 6.601977 6.971745 6.364667 7.209055
    ## [38,] 6.306094 6.128026 6.484162 5.886840 6.725348
    ## [39,] 6.600676 6.460387 6.740965 6.196020 7.005333
    ## [40,] 7.094493 6.934796 7.254189 6.682705 7.506280
    ## [41,] 6.595673 6.374613 6.816734 6.156431 7.034916
    ## [42,] 6.005732 5.761671 6.249794 5.554476 6.456988
    ## [43,] 6.962800 6.822918 7.102682 6.558285 7.367316
    ## [44,] 7.065421 6.910261 7.220580 6.655371 7.475470
    ## [45,] 6.266709 6.060228 6.473190 5.834621 6.698797
    ## [46,] 6.511698 6.315350 6.708046 6.084359 6.939037
    ## [47,] 6.823072 6.644370 7.001773 6.403548 7.242596

Which state has the highest predicted crime rate? the lowest?

``` r
# lowest 
best = which.min(BPM$fit)
crime.ZS$X[best, BPM$bestmodel]
```

    ## (Intercept)           M          So          Ed         Po1          LF 
    ##    1.000000    4.905275    0.000000    4.691348    4.234107    6.291569 
    ##         Pop          U1         GDP        Ineq 
    ##    1.791759    4.382027    6.335054    4.934474

What characteristics lead to the lowest rates? (where do the X values fall inthe distribution of the covariantes - are they at the extremes?)

Using the `newdata` option as with the `predict` function in `lm`, you can predict at new values of the covariates (OK in this case the data frame is the same, so it is the same as the insample prediction). The code below illustrates using BMA and Monte Carlo simulation to obtain the intervals.

``` r
BPM = predict(crime.ZS, UScrime, estimator="BMA", se.fit=TRUE, nsim=10000)
crime.conf.fit = confint(BPM, parm="mean")
crime.conf.pred = confint(BPM, parm="pred")
cbind(BPM$fit, crime.conf.fit, crime.conf.pred)
```

    ##                  2.5  %  97.5  %   2.5  %  97.5  %
    ##  [1,] 6.661770 6.512882 6.811075 6.234658 7.063577
    ##  [2,] 7.298827 7.132778 7.453672 6.886358 7.727224
    ##  [3,] 6.179308 5.958100 6.405988 5.717083 6.600490
    ##  [4,] 7.610585 7.368410 7.822511 7.155805 8.053656
    ##  [5,] 7.054238 6.844098 7.250001 6.630819 7.510648
    ##  [6,] 6.514064 6.292207 6.744759 6.075394 6.965791
    ##  [7,] 6.784846 6.504226 7.066619 6.318335 7.286166
    ##  [8,] 7.266344 7.037728 7.478964 6.805218 7.695829
    ##  [9,] 6.629448 6.479536 6.779708 6.209582 7.037250
    ## [10,] 6.601246 6.460499 6.735899 6.192440 7.004786
    ## [11,] 7.055003 6.867002 7.224986 6.606938 7.473345
    ## [12,] 6.570625 6.419642 6.713752 6.155398 6.990213
    ## [13,] 6.472327 6.211051 6.717664 5.986348 6.916874
    ## [14,] 6.582374 6.394368 6.763813 6.151168 7.002750
    ## [15,] 6.556880 6.360742 6.757767 6.122672 6.994393
    ## [16,] 6.905017 6.747085 7.068965 6.489052 7.322776
    ## [17,] 6.229073 6.000590 6.485469 5.782274 6.690349
    ## [18,] 6.809572 6.555233 7.111148 6.349449 7.293462
    ## [19,] 6.943294 6.750895 7.127736 6.514461 7.375024
    ## [20,] 6.961980 6.770795 7.137798 6.512600 7.376566
    ## [21,] 6.608947 6.398882 6.824247 6.156787 7.028210
    ## [22,] 6.429088 6.189743 6.667942 5.963231 6.864759
    ## [23,] 6.898828 6.688752 7.107624 6.461334 7.334837
    ## [24,] 6.777130 6.593948 6.950259 6.369596 7.208371
    ## [25,] 6.405741 6.201247 6.595429 5.998762 6.849421
    ## [26,] 7.401082 7.153332 7.665795 6.926678 7.865099
    ## [27,] 6.019651 5.784448 6.260335 5.556103 6.472612
    ## [28,] 7.156541 6.953589 7.341915 6.724304 7.596671
    ## [29,] 7.089698 6.867201 7.304004 6.664772 7.538180
    ## [30,] 6.500233 6.309000 6.683340 6.057258 6.911816
    ## [31,] 6.208963 6.003407 6.429731 5.788602 6.661988
    ## [32,] 6.605944 6.408841 6.793429 6.158235 7.027789
    ## [33,] 6.798139 6.637838 6.979892 6.381984 7.237230
    ## [34,] 6.820052 6.694118 6.949567 6.392036 7.206596
    ## [35,] 6.625465 6.418039 6.814910 6.186975 7.053399
    ## [36,] 7.029051 6.706463 7.324780 6.536249 7.519391
    ## [37,] 6.794004 6.543998 7.010721 6.341549 7.237754
    ## [38,] 6.363691 6.144696 6.602839 5.931563 6.838577
    ## [39,] 6.603108 6.454056 6.735558 6.179678 7.005518
    ## [40,] 7.044736 6.877766 7.212774 6.643930 7.477981
    ## [41,] 6.548160 6.305935 6.783977 6.086081 6.976145
    ## [42,] 6.046124 5.749605 6.330358 5.556594 6.534540
    ## [43,] 6.929741 6.749216 7.129373 6.507824 7.365301
    ## [44,] 7.006019 6.830917 7.179359 6.577869 7.411631
    ## [45,] 6.236002 5.998726 6.472859 5.797902 6.717865
    ## [46,] 6.608591 6.352199 6.842383 6.161208 7.067892
    ## [47,] 6.830450 6.654880 7.015341 6.404076 7.255819
