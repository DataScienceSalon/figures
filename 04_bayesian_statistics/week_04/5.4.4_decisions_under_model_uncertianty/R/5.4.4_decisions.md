decision making under uncertainty
================

Load the data and preprocess

``` r
library(MASS)
data(UScrime)

#Log transform all continuous variables except `So` which is in column 2. 
# We're overwriting the dataframe in this case.

UScrime[,-2] = log(UScrime[,-2])
```

### Run BAS

I am going to run `BAS` using the sampling without replacement option to enumerate all \(2^15\) models.

``` r
# library(devtools)
# install_github("merliseclyde/BAS")  # use version > 1.2.2
library(BAS)
crime.ZS =  bas.lm(y ~ ., 
                   data=UScrime,
                   prior="ZS-null",
                   modelprior=uniform()) 
```

**Model Choice**

`BAS` has methods defined to return fitted values, `fitted`, using the observed design matrix and predictions at either the observed data or potentially new values, `predict`, as with `lm`.

``` r
muhat.BMA = fitted(crime.ZS, estimator="BMA")
BMA  = predict(crime.ZS, estimator="BMA")

# predict has additional slots for fitted values under BMA, predictions under each model
names(BMA)
```

    ## [1] "fit"       "Ybma"      "Ypred"     "postprobs" "se.fit"    "se.pred"  
    ## [7] "best"      "bestmodel"

Plotting the two sets of fitted values,

``` r
par(mar=c(9, 9, 3, 3))
plot(muhat.BMA, BMA$fit, 
     pch=16, col=myblue,
     xlab=expression(hat(mu[i])), ylab=expression(hat(Y[i])))
abline(0,1)
```

![](5.4.4_decisions_files/figure-markdown_github/unnamed-chunk-1-1.png) we see that they are in perfect agreement. That is always the case as the posterior mean for the regression mean function at a point \(x\) is the expected posterior predictive value for \(Y\) at \(x\). This is true not only for estimators such as BMA, but the expected values under model selection.

### Inference with model selection

In addition to using BMA, we can use the posterior means under model selection. This corresponds to a decision rule that combines estimation and selection. `BAS` currently implements the following options

**highest probability model:**

``` r
HPM = predict(crime.ZS, estimator="HPM")

# show the indices of variables in the best model where 0 is the intercept
HPM$bestmodel
```

    ## [1]  0  1  3  4  9 11 13 14 15

A little more interpretable version with names:

``` r
(crime.ZS$namesx[HPM$bestmodel +1])[-1]
```

    ## [1] "M"    "Ed"   "Po1"  "NW"   "U2"   "Ineq" "Prob" "Time"

**median probability model:**

``` r
MPM = fitted(crime.ZS, estimator="MPM")
(crime.ZS$namesx[attr(MPM, 'model') +1])[-1]
```

    ## [1] "M"    "Ed"   "Po1"  "NW"   "U2"   "Ineq" "Prob"

Note that we can also extract the best model from the attribute in the fitted values as well.

**best predictive model:**

This is the model that is closest to BMA predictions under squared error loss.

``` r
BPM = fitted(crime.ZS, estimator="BPM")
(crime.ZS$namesx[attr(BPM, 'model') +1])[-1]
```

    ##  [1] "M"    "So"   "Ed"   "Po1"  "Po2"  "M.F"  "NW"   "U2"   "Ineq" "Prob"

Let's see how they compare:

``` r
myblue = rgb(86,155,189, name="myblue", max=256)
mydarkgrey = rgb(.5,.5,.5, name="mydarkgrey", max=1)
par(cex=1.8, cex.axis=1.8, cex.lab=2, mfrow=c(2,2), mar=c(5, 5, 3, 3), col.lab=mydarkgrey, col.axis=mydarkgrey, col=mydarkgrey)
library(GGally)
ggpairs(data.frame(HPM = as.vector(HPM$fit),  #this used predict so we need to extract fitted values
                   MPM = as.vector(MPM),  # this used fitted
                   BPM = as.vector(BPM),  # this used fitted
                   BMA = as.vector(BMA$fit))) # this used predict
```

![](5.4.4_decisions_files/figure-markdown_github/unnamed-chunk-6-1.png)
